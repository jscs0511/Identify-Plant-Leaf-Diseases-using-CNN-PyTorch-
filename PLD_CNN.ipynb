{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6h0oav1z4qs2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j687-K_HMtof",
    "outputId": "44a7ddc8-c045-45f8-9f26-5db9683dd5b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e5YClSZfEXs7",
    "outputId": "29293977-1323-47de-9f3f-a35d43a8b404"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "njvKcD-zLf8O"
   },
   "outputs": [],
   "source": [
    "shutil.rmtree('/content/drive/MyDrive/splitted')\n",
    "# force remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YCFEXfYIQg2e"
   },
   "outputs": [],
   "source": [
    "shutil.rmtree('/content/drive/MyDrive/dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l2opK1VzFFp8"
   },
   "outputs": [],
   "source": [
    "!unzip '/content/drive/MyDrive/dataset.zip' -d '/content/drive/MyDrive/dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lKw59vy9PgTz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "nRMjZNkx4tMm"
   },
   "outputs": [],
   "source": [
    "original_dataset_dir='/content/drive/MyDrive/dataset'\n",
    "#\n",
    "classes_list=os.listdir(original_dataset_dir)\n",
    "\n",
    "base_dir='/content/drive/MyDrive/splitted'\n",
    "os.mkdir(base_dir)\n",
    "\n",
    "#path.join join one or more path components intelligently. \n",
    "#This method concatenates various path components with exactly one directory separator \n",
    "#(‘/’) following each non-empty part except the last path component. \n",
    "train_dir=os.path.join(base_dir,'train') \n",
    "os.mkdir(train_dir)\n",
    "validation_dir=os.path.join(base_dir,'val')\n",
    "os.mkdir(validation_dir)\n",
    "test_dir=os.path.join(base_dir,'test')\n",
    "os.mkdir(test_dir)\n",
    "\n",
    "for clss in classes_list:\n",
    "  os.mkdir(os.path.join(train_dir,clss))\n",
    "  os.mkdir(os.path.join(validation_dir,clss))\n",
    "  os.mkdir(os.path.join(test_dir,clss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tS5OfDeCOsRY",
    "outputId": "63fa7b9b-4ae3-42e3-e456-b066b36ca738"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size( Pepper,_bell___healthy ):  886\n",
      "Validation size( Pepper,_bell___healthy ):  295\n",
      "Test size( Pepper,_bell___healthy ):  295\n",
      "Train size( Grape___Esca_(Black_Measles) ):  829\n",
      "Validation size( Grape___Esca_(Black_Measles) ):  276\n",
      "Test size( Grape___Esca_(Black_Measles) ):  276\n",
      "Train size( Pepper,_bell___Bacterial_spot ):  598\n",
      "Validation size( Pepper,_bell___Bacterial_spot ):  199\n",
      "Test size( Pepper,_bell___Bacterial_spot ):  199\n",
      "Train size( Strawberry___healthy ):  273\n",
      "Validation size( Strawberry___healthy ):  91\n",
      "Test size( Strawberry___healthy ):  91\n",
      "Train size( Grape___Black_rot ):  708\n",
      "Validation size( Grape___Black_rot ):  236\n",
      "Test size( Grape___Black_rot ):  236\n",
      "Train size( Corn___Common_rust ):  715\n",
      "Validation size( Corn___Common_rust ):  238\n",
      "Test size( Corn___Common_rust ):  238\n",
      "Train size( Apple___Apple_scab ):  378\n",
      "Validation size( Apple___Apple_scab ):  126\n",
      "Test size( Apple___Apple_scab ):  126\n",
      "Train size( Potato___healthy ):  91\n",
      "Validation size( Potato___healthy ):  30\n",
      "Test size( Potato___healthy ):  30\n",
      "Train size( Potato___Late_blight ):  600\n",
      "Validation size( Potato___Late_blight ):  200\n",
      "Test size( Potato___Late_blight ):  200\n",
      "Train size( Cherry___healthy ):  512\n",
      "Validation size( Cherry___healthy ):  170\n",
      "Test size( Cherry___healthy ):  170\n",
      "Train size( Tomato___Bacterial_spot ):  1276\n",
      "Validation size( Tomato___Bacterial_spot ):  425\n",
      "Test size( Tomato___Bacterial_spot ):  425\n",
      "Train size( Apple___Black_rot ):  372\n",
      "Validation size( Apple___Black_rot ):  124\n",
      "Test size( Apple___Black_rot ):  124\n",
      "Train size( Cherry___Powdery_mildew ):  631\n",
      "Validation size( Cherry___Powdery_mildew ):  210\n",
      "Test size( Cherry___Powdery_mildew ):  210\n",
      "Train size( Corn___Cercospora_leaf_spot Gray_leaf_spot ):  307\n",
      "Validation size( Corn___Cercospora_leaf_spot Gray_leaf_spot ):  102\n",
      "Test size( Corn___Cercospora_leaf_spot Gray_leaf_spot ):  102\n",
      "Train size( Peach___healthy ):  216\n",
      "Validation size( Peach___healthy ):  72\n",
      "Test size( Peach___healthy ):  72\n",
      "Train size( Tomato___Early_blight ):  600\n",
      "Validation size( Tomato___Early_blight ):  200\n",
      "Test size( Tomato___Early_blight ):  200\n",
      "Train size( Tomato___Tomato_Yellow_Leaf_Curl_Virus ):  3214\n",
      "Validation size( Tomato___Tomato_Yellow_Leaf_Curl_Virus ):  1071\n",
      "Test size( Tomato___Tomato_Yellow_Leaf_Curl_Virus ):  1071\n",
      "Train size( Grape___Leaf_blight_(Isariopsis_Leaf_Spot) ):  645\n",
      "Validation size( Grape___Leaf_blight_(Isariopsis_Leaf_Spot) ):  215\n",
      "Test size( Grape___Leaf_blight_(Isariopsis_Leaf_Spot) ):  215\n",
      "Train size( Potato___Early_blight ):  600\n",
      "Validation size( Potato___Early_blight ):  200\n",
      "Test size( Potato___Early_blight ):  200\n",
      "Train size( Grape___healthy ):  253\n",
      "Validation size( Grape___healthy ):  84\n",
      "Test size( Grape___healthy ):  84\n",
      "Train size( Apple___Cedar_apple_rust ):  165\n",
      "Validation size( Apple___Cedar_apple_rust ):  55\n",
      "Test size( Apple___Cedar_apple_rust ):  55\n",
      "Train size( Corn___Northern_Leaf_Blight ):  591\n",
      "Validation size( Corn___Northern_Leaf_Blight ):  197\n",
      "Test size( Corn___Northern_Leaf_Blight ):  197\n",
      "Train size( Tomato___Septoria_leaf_spot ):  1062\n",
      "Validation size( Tomato___Septoria_leaf_spot ):  354\n",
      "Test size( Tomato___Septoria_leaf_spot ):  354\n",
      "Train size( Corn___healthy ):  697\n",
      "Validation size( Corn___healthy ):  232\n",
      "Test size( Corn___healthy ):  232\n",
      "Train size( Strawberry___Leaf_scorch ):  665\n",
      "Validation size( Strawberry___Leaf_scorch ):  221\n",
      "Test size( Strawberry___Leaf_scorch ):  221\n",
      "Train size( Tomato___Tomato_mosaic_virus ):  223\n",
      "Validation size( Tomato___Tomato_mosaic_virus ):  74\n",
      "Test size( Tomato___Tomato_mosaic_virus ):  74\n",
      "Train size( Tomato___Leaf_Mold ):  571\n",
      "Validation size( Tomato___Leaf_Mold ):  190\n",
      "Test size( Tomato___Leaf_Mold ):  190\n",
      "Train size( Tomato___Late_blight ):  1145\n",
      "Validation size( Tomato___Late_blight ):  381\n",
      "Test size( Tomato___Late_blight ):  381\n",
      "Train size( Tomato___healthy ):  954\n",
      "Validation size( Tomato___healthy ):  318\n",
      "Test size( Tomato___healthy ):  318\n",
      "Train size( Tomato___Spider_mites Two-spotted_spider_mite ):  1005\n",
      "Validation size( Tomato___Spider_mites Two-spotted_spider_mite ):  335\n",
      "Test size( Tomato___Spider_mites Two-spotted_spider_mite ):  335\n",
      "Train size( Apple___healthy ):  987\n",
      "Validation size( Apple___healthy ):  329\n",
      "Test size( Apple___healthy ):  329\n",
      "Train size( Tomato___Target_Spot ):  842\n",
      "Validation size( Tomato___Target_Spot ):  280\n",
      "Test size( Tomato___Target_Spot ):  280\n",
      "Train size( Peach___Bacterial_spot ):  1378\n",
      "Validation size( Peach___Bacterial_spot ):  459\n",
      "Test size( Peach___Bacterial_spot ):  459\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    " \n",
    "for cls in classes_list:\n",
    "    path = os.path.join(original_dataset_dir, cls)\n",
    "    fnames = os.listdir(path)\n",
    " \n",
    "    train_size = math.floor(len(fnames) * 0.6)\n",
    "    validation_size = math.floor(len(fnames) * 0.2)\n",
    "    test_size = math.floor(len(fnames) * 0.2)\n",
    "    \n",
    "\n",
    "    #split dataset into train, validation, and test set\n",
    "    train_fnames = fnames[:train_size]\n",
    "    print(\"Train size(\",cls,\"): \", len(train_fnames))\n",
    "    for fname in train_fnames:\n",
    "        src = os.path.join(path, fname)\n",
    "        dst = os.path.join(os.path.join(train_dir, cls), fname)\n",
    "        shutil.copyfile(src, dst)\n",
    "        \n",
    "    validation_fnames = fnames[train_size:(validation_size + train_size)]\n",
    "    print(\"Validation size(\",cls,\"): \", len(validation_fnames))\n",
    "    for fname in validation_fnames:\n",
    "        src = os.path.join(path, fname)\n",
    "        dst = os.path.join(os.path.join(validation_dir, cls), fname)\n",
    "        shutil.copyfile(src, dst)\n",
    "        \n",
    "    test_fnames = fnames[(train_size+validation_size):(validation_size + train_size +test_size)]\n",
    "\n",
    "    print(\"Test size(\",cls,\"): \", len(test_fnames))\n",
    "    for fname in test_fnames:\n",
    "        src = os.path.join(path, fname)\n",
    "        dst = os.path.join(os.path.join(test_dir, cls), fname)\n",
    "        shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "fB5cOuW-Ot5Y"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "#cuda setting\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "BATCH_SIZE = 256 \n",
    "EPOCH = 30 #dataset iteration count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NPCc1efWOtwS",
    "outputId": "d2a2d9f5-abb5-4f69-8352-0aac01c7935c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(USE_CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "AVNG9aoYSy9H"
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder \n",
    "\n",
    "#set the way how image will be preprocessed\n",
    "transform_base = transforms.Compose([transforms.Resize((64,64)), transforms.ToTensor()]) \n",
    "\n",
    "#load image dataset from the root folder(transform option: preprocessing method)\n",
    "train_dataset = ImageFolder(root='/content/drive/MyDrive/splitted/train', transform=transform_base) \n",
    "val_dataset = ImageFolder(root='/content/drive/MyDrive/splitted/val', transform=transform_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0PyfBbcNU0e-",
    "outputId": "c39dda7d-9094-40cd-d593-ef1b05a08d8b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#divide image dataset up into the size of minibatch\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "p4_4kcCDU8Tn"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    " \n",
    "class Net(nn.Module): \n",
    "  \n",
    "    def __init__(self): \n",
    "    \n",
    "        super(Net, self).__init__() \n",
    "        \n",
    "        #input channel: 3(color image), output channel: 32, filter size:3, padding(L,R,U,D):1 \n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1) \n",
    "\n",
    "        #first: conv, next:max pooling\n",
    "        self.pool = nn.MaxPool2d(2,2)  \n",
    "\n",
    "        #the number of input channel should be same with the number of previous output channel\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)  \n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, padding=1)  \n",
    "        \n",
    "        #image size\n",
    "        #conv1 -> max pooling: 32\n",
    "        #conv2 -> max pooling: 16\n",
    "        #conv3 -> max pooling: 8(final size of the image)\n",
    "        #Thus, 64(output filter) * 8 (width) * 8(height) = 4096\n",
    "        self.fc1 = nn.Linear(4096, 512) \n",
    "        self.fc2 = nn.Linear(512, 33) \n",
    "    \n",
    "    def forward(self, x):  \n",
    "    \n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)  \n",
    "        x = self.pool(x) \n",
    "        x = F.dropout(x, p=0.25, training=self.training) \n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x) \n",
    "        x = self.pool(x) \n",
    "        x = F.dropout(x, p=0.25, training=self.training)\n",
    "\n",
    "        x = self.conv3(x) \n",
    "        x = F.relu(x) \n",
    "        x = self.pool(x) \n",
    "        x = F.dropout(x, p=0.25, training=self.training)\n",
    "\n",
    "        #flatten dataset to 1 by 4096\n",
    "        x = x.view(-1, 4096)  \n",
    "        x = self.fc1(x) \n",
    "        x = F.relu(x) \n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.fc2(x) \n",
    "\n",
    "        return F.log_softmax(x, dim=1)  \n",
    "\n",
    "model_base = Net().to(DEVICE)  \n",
    "optimizer = optim.Adam(model_base.parameters(), lr=0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "THcUp1TAVFc0"
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer):\n",
    "    model.train()  \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE) \n",
    "        optimizer.zero_grad() #initialize previous gradients\n",
    "        output = model(data) \n",
    "        loss = F.cross_entropy(output, target) \n",
    "        loss.backward() #back propagate to get new gradient in each parameter\n",
    "        optimizer.step() #update model's parameter with the new gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "MbhGxVhTVGxL"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()  \n",
    "    test_loss = 0 \n",
    "    correct = 0   \n",
    "    \n",
    "    with torch.no_grad():#prevent parameter from updating \n",
    "        for data, target in test_loader:  \n",
    "            data, target = data.to(DEVICE), target.to(DEVICE) #allocate CUDA device \n",
    "            output = model(data) \n",
    "            \n",
    "            #measure loss with cross_entropy function(softmax which consists of exponential)\n",
    "            #softmax--> normalize a value between 0 and 1\n",
    "            #item()--> return a value from tensor type\n",
    "            test_loss += F.cross_entropy(output,target, reduction='sum').item() \n",
    "            \n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            \n",
    "            #compare target with prediction as they have same shape after using view_as()\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item() \n",
    "   \n",
    "    test_loss /= len(test_loader.dataset) \n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset) \n",
    "    return test_loss, test_accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LChz5CvQVGqT",
    "outputId": "3a5eca99-c124-4d3a-9426-36a3d764a299"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- epoch 1 ----------------\n",
      "train Loss: 1.6174, Accuracy: 54.79%\n",
      "val Loss: 1.6486, Accuracy: 53.49%\n",
      "Completed in 2m 23s\n",
      "-------------- epoch 2 ----------------\n",
      "train Loss: 0.9491, Accuracy: 72.12%\n",
      "val Loss: 0.9908, Accuracy: 70.46%\n",
      "Completed in 2m 18s\n",
      "-------------- epoch 3 ----------------\n",
      "train Loss: 0.7374, Accuracy: 77.26%\n",
      "val Loss: 0.7948, Accuracy: 75.45%\n",
      "Completed in 2m 18s\n",
      "-------------- epoch 4 ----------------\n",
      "train Loss: 0.5954, Accuracy: 81.62%\n",
      "val Loss: 0.6567, Accuracy: 80.00%\n",
      "Completed in 2m 17s\n",
      "-------------- epoch 5 ----------------\n",
      "train Loss: 0.5056, Accuracy: 84.31%\n",
      "val Loss: 0.5709, Accuracy: 81.81%\n",
      "Completed in 2m 20s\n",
      "-------------- epoch 6 ----------------\n",
      "train Loss: 0.4320, Accuracy: 86.38%\n",
      "val Loss: 0.5074, Accuracy: 83.61%\n",
      "Completed in 2m 19s\n",
      "-------------- epoch 7 ----------------\n",
      "train Loss: 0.3856, Accuracy: 87.99%\n",
      "val Loss: 0.4626, Accuracy: 85.25%\n",
      "Completed in 2m 20s\n",
      "-------------- epoch 8 ----------------\n",
      "train Loss: 0.4326, Accuracy: 85.44%\n",
      "val Loss: 0.5290, Accuracy: 82.75%\n",
      "Completed in 2m 20s\n",
      "-------------- epoch 9 ----------------\n",
      "train Loss: 0.3565, Accuracy: 88.89%\n",
      "val Loss: 0.4521, Accuracy: 85.69%\n",
      "Completed in 2m 25s\n",
      "-------------- epoch 10 ----------------\n",
      "train Loss: 0.3198, Accuracy: 90.21%\n",
      "val Loss: 0.4270, Accuracy: 86.68%\n",
      "Completed in 2m 24s\n",
      "-------------- epoch 11 ----------------\n",
      "train Loss: 0.2832, Accuracy: 90.95%\n",
      "val Loss: 0.3982, Accuracy: 87.08%\n",
      "Completed in 2m 23s\n",
      "-------------- epoch 12 ----------------\n",
      "train Loss: 0.2731, Accuracy: 91.33%\n",
      "val Loss: 0.3888, Accuracy: 87.51%\n",
      "Completed in 2m 24s\n",
      "-------------- epoch 13 ----------------\n",
      "train Loss: 0.2361, Accuracy: 92.94%\n",
      "val Loss: 0.3465, Accuracy: 89.15%\n",
      "Completed in 3m 7s\n",
      "-------------- epoch 14 ----------------\n",
      "train Loss: 0.2462, Accuracy: 92.28%\n",
      "val Loss: 0.3748, Accuracy: 88.33%\n",
      "Completed in 2m 35s\n",
      "-------------- epoch 15 ----------------\n",
      "train Loss: 0.2314, Accuracy: 92.75%\n",
      "val Loss: 0.3658, Accuracy: 88.31%\n",
      "Completed in 2m 25s\n",
      "-------------- epoch 16 ----------------\n",
      "train Loss: 0.1727, Accuracy: 94.83%\n",
      "val Loss: 0.3102, Accuracy: 90.19%\n",
      "Completed in 2m 24s\n",
      "-------------- epoch 17 ----------------\n",
      "train Loss: 0.1516, Accuracy: 95.72%\n",
      "val Loss: 0.2881, Accuracy: 91.26%\n",
      "Completed in 2m 18s\n",
      "-------------- epoch 18 ----------------\n",
      "train Loss: 0.1364, Accuracy: 96.13%\n",
      "val Loss: 0.2722, Accuracy: 91.78%\n",
      "Completed in 2m 15s\n",
      "-------------- epoch 19 ----------------\n",
      "train Loss: 0.1512, Accuracy: 95.42%\n",
      "val Loss: 0.2938, Accuracy: 90.60%\n",
      "Completed in 2m 32s\n",
      "-------------- epoch 20 ----------------\n",
      "train Loss: 0.1314, Accuracy: 96.22%\n",
      "val Loss: 0.2719, Accuracy: 91.79%\n",
      "Completed in 2m 18s\n",
      "-------------- epoch 21 ----------------\n",
      "train Loss: 0.1280, Accuracy: 96.16%\n",
      "val Loss: 0.2838, Accuracy: 91.61%\n",
      "Completed in 2m 18s\n",
      "-------------- epoch 22 ----------------\n",
      "train Loss: 0.1011, Accuracy: 97.18%\n",
      "val Loss: 0.2461, Accuracy: 92.71%\n",
      "Completed in 2m 29s\n",
      "-------------- epoch 23 ----------------\n",
      "train Loss: 0.0925, Accuracy: 97.53%\n",
      "val Loss: 0.2509, Accuracy: 92.50%\n",
      "Completed in 2m 21s\n",
      "-------------- epoch 24 ----------------\n",
      "train Loss: 0.0974, Accuracy: 97.46%\n",
      "val Loss: 0.2518, Accuracy: 92.20%\n",
      "Completed in 2m 18s\n",
      "-------------- epoch 25 ----------------\n",
      "train Loss: 0.0813, Accuracy: 97.93%\n",
      "val Loss: 0.2345, Accuracy: 93.04%\n",
      "Completed in 2m 17s\n",
      "-------------- epoch 26 ----------------\n",
      "train Loss: 0.0872, Accuracy: 97.70%\n",
      "val Loss: 0.2442, Accuracy: 92.75%\n",
      "Completed in 2m 16s\n",
      "-------------- epoch 27 ----------------\n",
      "train Loss: 0.0832, Accuracy: 97.59%\n",
      "val Loss: 0.2493, Accuracy: 92.44%\n",
      "Completed in 2m 17s\n",
      "-------------- epoch 28 ----------------\n",
      "train Loss: 0.0605, Accuracy: 98.53%\n",
      "val Loss: 0.2208, Accuracy: 93.70%\n",
      "Completed in 2m 25s\n",
      "-------------- epoch 29 ----------------\n",
      "train Loss: 0.0624, Accuracy: 98.55%\n",
      "val Loss: 0.2223, Accuracy: 93.75%\n",
      "Completed in 2m 20s\n",
      "-------------- epoch 30 ----------------\n",
      "train Loss: 0.0807, Accuracy: 97.70%\n",
      "val Loss: 0.2602, Accuracy: 92.50%\n",
      "Completed in 2m 18s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import copy\n",
    " \n",
    "def train_baseline(model ,train_loader, val_loader, optimizer, num_epochs = 30):\n",
    "    best_acc = 0.0  \n",
    "    best_model_wts = copy.deepcopy(model.state_dict()) \n",
    " \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        since = time.time()  #measure elasped time\n",
    "        train(model, train_loader, optimizer) #train model with dataset and optimizer\n",
    "        train_loss, train_acc = evaluate(model, train_loader)  #get train loss and accuracy\n",
    "        val_loss, val_acc = evaluate(model, val_loader) #get validation loss and accuracy\n",
    "        \n",
    "        if val_acc > best_acc: #if validation accuary is higher than previous best one, then change the value\n",
    "            best_acc = val_acc \n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "        time_elapsed = time.time() - since \n",
    "        print('-------------- epoch {} ----------------'.format(epoch))\n",
    "        print('train Loss: {:.4f}, Accuracy: {:.2f}%'.format(train_loss, train_acc))   \n",
    "        print('val Loss: {:.4f}, Accuracy: {:.2f}%'.format(val_loss, val_acc))\n",
    "        print('Completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)) \n",
    "    model.load_state_dict(best_model_wts)  \n",
    "    return model\n",
    " \n",
    "\n",
    "base = train_baseline(model_base, train_loader, val_loader, optimizer, EPOCH)  \t #(16)\n",
    "torch.save(base,'baseline.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "9oFV-IKhVGnx"
   },
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([transforms.Resize([64,64]), \n",
    "        transforms.RandomHorizontalFlip(), transforms.RandomVerticalFlip(),  \n",
    "        transforms.RandomCrop(52), transforms.ToTensor(), \n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]),\n",
    "    \n",
    "    'val': transforms.Compose([transforms.Resize([64,64]),  \n",
    "        transforms.RandomCrop(52), transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rqJp47M_VGlt",
    "outputId": "c94438a5-bad7-4586-d003-801e19e52241"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/content/drive/MyDrive/splitted' \n",
    "image_datasets = {x: ImageFolder(root=os.path.join(data_dir, x), transform=data_transforms[x]) for x in ['train', 'val']} \n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=BATCH_SIZE, shuffle=True, num_workers=4) for x in ['train', 'val']} \n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "\n",
    "class_names = image_datasets['train'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "referenced_widgets": [
      "182dd55f82c645c096d60c8af22881a8",
      "0b71fe82f5524c718bc4a4a82011f82d",
      "06357d094eee48ea826ad310e029dc66",
      "2150a9da246c48fd9a816f3decdb1503",
      "cd96f720dc2d4a4caf7a03b24d3328dd",
      "e284eadf251d46828f667a4661ed2389",
      "fc69d1f4117648ee81ca6398242dd4ac",
      "f98e137d061c45caa2854b6a1edfe91c",
      "5075c22354114553990dcdb5474648e1",
      "83ac045cd55f40e59e4ef5d6e5430eff",
      "74a114bafd204af88bbbcd110082047f"
     ]
    },
    "id": "kXcbC0jtVGis",
    "outputId": "92c33737-22a5-44fa-fcf5-b9d924cafee0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "182dd55f82c645c096d60c8af22881a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchvision import models\n",
    " \n",
    "resnet = models.resnet50(pretrained=True)  #True: use pretrained parameters, False: parameters will be set randomly\n",
    "num_ftrs = resnet.fc.in_features   \n",
    "resnet.fc = nn.Linear(num_ftrs, 33) \n",
    "resnet = resnet.to(DEVICE)\n",
    " \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "#among parameters in ResNet model, if a parameter is set as true to update,\n",
    "#update the parameter with Adam optimizer.\n",
    "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, resnet.parameters()), lr=0.001)\n",
    " \n",
    "from torch.optim import lr_scheduler\n",
    "#reduce learning rate as 7 epoch has passed.\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "2ICYICFIVGge"
   },
   "outputs": [],
   "source": [
    "ct = 0 \n",
    "#freeze layers before 6th(total 10 layers)\n",
    "#1~5 layers: freeze\n",
    "#6~10: don't freeze\n",
    "for child in resnet.children():\n",
    "    ct += 1  \n",
    "    #print(ct) --> check current layer if uncommented  \n",
    "    if ct < 6: \n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "bwmUyh0dVGdp"
   },
   "outputs": [],
   "source": [
    "def train_resnet(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())  \n",
    "    best_acc = 0.0  \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('-------------- epoch {} ----------------'.format(epoch+1)) \n",
    "        since = time.time()                                     \n",
    "        for phase in ['train', 'val']: \n",
    "            if phase == 'train': \n",
    "                model.train() \n",
    "            else:\n",
    "                model.eval()     \n",
    " \n",
    "            running_loss = 0.0  \n",
    "            running_corrects = 0  \n",
    " \n",
    "            \n",
    "            for inputs, labels in dataloaders[phase]: \n",
    "                inputs = inputs.to(DEVICE)  \n",
    "                labels = labels.to(DEVICE)  \n",
    "                \n",
    "                optimizer.zero_grad() \n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):  \n",
    "                    outputs = model(inputs)  \n",
    "                    _, preds = torch.max(outputs, 1) \n",
    "                    loss = criterion(outputs, labels)  \n",
    "    \n",
    "                    if phase == 'train':   \n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    " \n",
    "                running_loss += loss.item() * inputs.size(0)  \n",
    "                running_corrects += torch.sum(preds == labels.data)  \n",
    "            if phase == 'train':  \n",
    "                scheduler.step()\n",
    " \n",
    "            epoch_loss = running_loss/dataset_sizes[phase]  \n",
    "            epoch_acc = running_corrects.double()/dataset_sizes[phase]  \n",
    " \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc)) \n",
    " \n",
    "          \n",
    "            if phase == 'val' and epoch_acc > best_acc: \n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    " \n",
    "        time_elapsed = time.time() - since  \n",
    "        print('Completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    " \n",
    "    model.load_state_dict(best_model_wts) \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GHRaW0XiVGbe",
    "outputId": "221a64e2-89a9-4a93-a69e-1343428839ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- epoch 1 ----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5849 Acc: 0.8249\n",
      "val Loss: 0.3381 Acc: 0.8975\n",
      "Completed in 1m 39s\n",
      "-------------- epoch 2 ----------------\n",
      "train Loss: 0.2262 Acc: 0.9273\n",
      "val Loss: 0.2530 Acc: 0.9179\n",
      "Completed in 1m 39s\n",
      "-------------- epoch 3 ----------------\n",
      "train Loss: 0.1720 Acc: 0.9451\n",
      "val Loss: 0.1812 Acc: 0.9462\n",
      "Completed in 1m 39s\n",
      "-------------- epoch 4 ----------------\n",
      "train Loss: 0.1402 Acc: 0.9555\n",
      "val Loss: 0.2001 Acc: 0.9387\n",
      "Completed in 1m 38s\n",
      "-------------- epoch 5 ----------------\n",
      "train Loss: 0.1155 Acc: 0.9631\n",
      "val Loss: 0.1318 Acc: 0.9574\n",
      "Completed in 1m 37s\n",
      "-------------- epoch 6 ----------------\n",
      "train Loss: 0.0933 Acc: 0.9689\n",
      "val Loss: 0.1312 Acc: 0.9604\n",
      "Completed in 1m 39s\n",
      "-------------- epoch 7 ----------------\n",
      "train Loss: 0.0911 Acc: 0.9700\n",
      "val Loss: 0.1487 Acc: 0.9592\n",
      "Completed in 1m 40s\n",
      "-------------- epoch 8 ----------------\n",
      "train Loss: 0.0521 Acc: 0.9834\n",
      "val Loss: 0.0484 Acc: 0.9850\n",
      "Completed in 1m 40s\n",
      "-------------- epoch 9 ----------------\n",
      "train Loss: 0.0289 Acc: 0.9911\n",
      "val Loss: 0.0445 Acc: 0.9859\n",
      "Completed in 1m 38s\n",
      "-------------- epoch 10 ----------------\n",
      "train Loss: 0.0249 Acc: 0.9925\n",
      "val Loss: 0.0439 Acc: 0.9856\n",
      "Completed in 1m 39s\n",
      "-------------- epoch 11 ----------------\n",
      "train Loss: 0.0207 Acc: 0.9931\n",
      "val Loss: 0.0403 Acc: 0.9870\n",
      "Completed in 1m 38s\n",
      "-------------- epoch 12 ----------------\n",
      "train Loss: 0.0200 Acc: 0.9932\n",
      "val Loss: 0.0364 Acc: 0.9885\n",
      "Completed in 1m 38s\n",
      "-------------- epoch 13 ----------------\n",
      "train Loss: 0.0185 Acc: 0.9941\n",
      "val Loss: 0.0368 Acc: 0.9889\n",
      "Completed in 1m 39s\n",
      "-------------- epoch 14 ----------------\n",
      "train Loss: 0.0168 Acc: 0.9940\n",
      "val Loss: 0.0364 Acc: 0.9886\n",
      "Completed in 1m 37s\n",
      "-------------- epoch 15 ----------------\n",
      "train Loss: 0.0145 Acc: 0.9953\n",
      "val Loss: 0.0358 Acc: 0.9885\n",
      "Completed in 1m 37s\n",
      "-------------- epoch 16 ----------------\n",
      "train Loss: 0.0138 Acc: 0.9959\n",
      "val Loss: 0.0374 Acc: 0.9886\n",
      "Completed in 1m 42s\n",
      "-------------- epoch 17 ----------------\n",
      "train Loss: 0.0130 Acc: 0.9956\n",
      "val Loss: 0.0357 Acc: 0.9885\n",
      "Completed in 1m 45s\n",
      "-------------- epoch 18 ----------------\n",
      "train Loss: 0.0128 Acc: 0.9961\n",
      "val Loss: 0.0387 Acc: 0.9886\n",
      "Completed in 1m 40s\n",
      "-------------- epoch 19 ----------------\n",
      "train Loss: 0.0150 Acc: 0.9950\n",
      "val Loss: 0.0328 Acc: 0.9895\n",
      "Completed in 1m 44s\n",
      "-------------- epoch 20 ----------------\n",
      "train Loss: 0.0132 Acc: 0.9957\n",
      "val Loss: 0.0320 Acc: 0.9910\n",
      "Completed in 1m 46s\n",
      "-------------- epoch 21 ----------------\n",
      "train Loss: 0.0125 Acc: 0.9960\n",
      "val Loss: 0.0335 Acc: 0.9885\n",
      "Completed in 1m 39s\n",
      "-------------- epoch 22 ----------------\n",
      "train Loss: 0.0126 Acc: 0.9959\n",
      "val Loss: 0.0342 Acc: 0.9894\n",
      "Completed in 1m 40s\n",
      "-------------- epoch 23 ----------------\n",
      "train Loss: 0.0115 Acc: 0.9965\n",
      "val Loss: 0.0300 Acc: 0.9901\n",
      "Completed in 1m 43s\n",
      "-------------- epoch 24 ----------------\n",
      "train Loss: 0.0113 Acc: 0.9963\n",
      "val Loss: 0.0356 Acc: 0.9897\n",
      "Completed in 1m 39s\n",
      "-------------- epoch 25 ----------------\n",
      "train Loss: 0.0114 Acc: 0.9967\n",
      "val Loss: 0.0335 Acc: 0.9902\n",
      "Completed in 1m 40s\n",
      "-------------- epoch 26 ----------------\n",
      "train Loss: 0.0119 Acc: 0.9962\n",
      "val Loss: 0.0322 Acc: 0.9892\n",
      "Completed in 1m 38s\n",
      "-------------- epoch 27 ----------------\n",
      "train Loss: 0.0116 Acc: 0.9965\n",
      "val Loss: 0.0337 Acc: 0.9899\n",
      "Completed in 1m 40s\n",
      "-------------- epoch 28 ----------------\n",
      "train Loss: 0.0116 Acc: 0.9962\n",
      "val Loss: 0.0366 Acc: 0.9885\n",
      "Completed in 1m 38s\n",
      "-------------- epoch 29 ----------------\n",
      "train Loss: 0.0123 Acc: 0.9958\n",
      "val Loss: 0.0365 Acc: 0.9891\n",
      "Completed in 1m 46s\n",
      "-------------- epoch 30 ----------------\n",
      "train Loss: 0.0125 Acc: 0.9960\n",
      "val Loss: 0.0370 Acc: 0.9892\n",
      "Completed in 1m 39s\n",
      "Best val Acc: 0.990988\n"
     ]
    }
   ],
   "source": [
    "model_resnet50 = train_resnet(resnet, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=EPOCH) \n",
    "\n",
    "torch.save(model_resnet50, 'resnet50.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NUe0MfwEVGYQ",
    "outputId": "eecc7a7e-e6e3-4310-e5ab-e1689fff2c02"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ],
   "source": [
    "transform_base = transforms.Compose([transforms.Resize([64,64]),transforms.ToTensor()])\n",
    "test_base = ImageFolder(root='/content/drive/MyDrive/splitted/test',transform=transform_base)  \n",
    "test_loader_base = torch.utils.data.DataLoader(test_base, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qxkuZLnFVk4i",
    "outputId": "1f150013-fd29-405d-e3cc-312c3b2818e8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ],
   "source": [
    "transform_resNet = transforms.Compose([\n",
    "        transforms.Resize([64,64]),  \n",
    "        transforms.RandomCrop(52),  \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
    "        #first list: average value while applying to normalization in each channel(RGB)\n",
    "        #second list: standard deviation while applying to normalization in each channel(RGB)\n",
    "    ])\n",
    "    \n",
    "test_resNet = ImageFolder(root='/content/drive/MyDrive/splitted/test', transform=transform_resNet) \n",
    "test_loader_resNet = torch.utils.data.DataLoader(test_resNet, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QuTd97GoVk16",
    "outputId": "89c526ec-2634-4a6b-c0fb-88ef1f42f9f8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline test acc:   93.50356740518212\n"
     ]
    }
   ],
   "source": [
    "baseline=torch.load('baseline.pt') \n",
    "baseline.eval()  \n",
    "test_loss, test_accuracy = evaluate(baseline, test_loader_base)\n",
    "\n",
    "print('baseline test acc:  ', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UvpPKbN2Vkyi",
    "outputId": "f685a55a-4f9b-4a4b-fb3a-d4804c594f77"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet test acc:   98.84841657278758\n"
     ]
    }
   ],
   "source": [
    "resnet50=torch.load('resnet50.pt') \n",
    "resnet50.eval()  \n",
    "test_loss, test_accuracy = evaluate(resnet50, test_loader_resNet)\n",
    "\n",
    "print('ResNet test acc:  ', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5CVDerKZVkvy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3hYOU8j2VktX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wyOveKN3VkrC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled8.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "06357d094eee48ea826ad310e029dc66": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f98e137d061c45caa2854b6a1edfe91c",
      "max": 102530333,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5075c22354114553990dcdb5474648e1",
      "value": 102530333
     }
    },
    "0b71fe82f5524c718bc4a4a82011f82d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e284eadf251d46828f667a4661ed2389",
      "placeholder": "​",
      "style": "IPY_MODEL_fc69d1f4117648ee81ca6398242dd4ac",
      "value": "100%"
     }
    },
    "182dd55f82c645c096d60c8af22881a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0b71fe82f5524c718bc4a4a82011f82d",
       "IPY_MODEL_06357d094eee48ea826ad310e029dc66",
       "IPY_MODEL_2150a9da246c48fd9a816f3decdb1503"
      ],
      "layout": "IPY_MODEL_cd96f720dc2d4a4caf7a03b24d3328dd"
     }
    },
    "2150a9da246c48fd9a816f3decdb1503": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_83ac045cd55f40e59e4ef5d6e5430eff",
      "placeholder": "​",
      "style": "IPY_MODEL_74a114bafd204af88bbbcd110082047f",
      "value": " 97.8M/97.8M [00:02&lt;00:00, 136MB/s]"
     }
    },
    "5075c22354114553990dcdb5474648e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "74a114bafd204af88bbbcd110082047f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "83ac045cd55f40e59e4ef5d6e5430eff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cd96f720dc2d4a4caf7a03b24d3328dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e284eadf251d46828f667a4661ed2389": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f98e137d061c45caa2854b6a1edfe91c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc69d1f4117648ee81ca6398242dd4ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
